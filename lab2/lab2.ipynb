{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Exploratory Data Analysis (Python, Pandas & matplotlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab 2, we are going to learn how to explore data using Python. Namely, we will work with Pandas and matplotlib, for data filtering and grouping, and visualization respectively. Provided VM is recommeded for this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download this notebook so you can edit it. (If you are viewing it via nbviewer.ipython.org, then use the link in the upper right corner.) To edit this notebook, in your VM terminal, type \"ipython notebook\" and in your prompted brower, click the notebook file to open and edit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the sampled data from the log file in lab 1, download the file 'wc_day6_1_sample.csv' [here](https://ufl.instructure.com/courses/344204/files/folder/Labs/Lab%202) and put it in your VM (To enable the 'Drag'n Drop' for ease of transfering files to/from VM, go to Device -> Drag 'n Drop -> Bidirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main data structures used by pandas are\n",
    "\n",
    " * Series: one-dimensional collections of any data type.\n",
    " * DataFrames: two-dimensional data structures similar to a database table.\n",
    "\n",
    "At first, we need to import the libraries we need. In the code cell below, we will import two libraries, pandas for data processing and PyLab for plotting.\n",
    "\n",
    "To run the code in the cell below, select the cell, then press 'ctrl + enter', or hit the 'play' button above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our first DataFrame using pandas, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame( { 'a' : [1, 2, 3, 4], 'b': [ 'w', 'x', 'y', 'z'] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is going to create a table with two columns and four rows, we can inspect the DataFrame by typing the name on the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detailed information about the schema of a DataFrame we can use the info function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the first few rows we can use head and the last few rows tail functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access any range of rows in the DataFrame we can use array-like indexes, for example, the following will retrieve rows 1 and 2 from our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following exercises we will use the dataset mentioned in setup.\n",
    "\n",
    "After you download this CSV file, we can import it into a DataFrame, assuming the file is in the path /home/datascience/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_df = pd.read_csv(\"/home/datascience/labs/notebooks/wc_day6_1_sample.csv\",\n",
    "                     names=['ClientID', 'Date', 'Time', 'URL', 'ResponseCode', 'Size'],\n",
    "                     na_values=['-'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using read_csv function, we created a new DataFrame with column names passed as a list in the names parameter and null or non existing values represented with \"-\" in the na_values parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create row filters, we use lists of Boolean values that evaluate a condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_may1st = log_df['Date'] == '01/May/1998'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a list of True/False values for each row in **log_df** that evaluate the given condition. To filter using this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "may1_df = log_df[is_may1st]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done more concisely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "may1_df = log_df[log_df['Date'] == '01/May/1998']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For column filters, we use the name of the columns that we want to keep passes as a string array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_codes = log_df[['URL', 'ResponseCode']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For grouping by a column, i.e. dividing all the rows into groups of the same value in that column, we use **groupby** which does not return a DataFrame but a **DataFrameGroupBy** object. The **DataFrameGroupBy** object is a dictionary-like object where keys are the distinct values in the grouping column, and values are DataFrames containnig th remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = log_df.groupby('ResponseCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.get_group(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group by multiple columns\n",
    "multi_grouped = log_df.groupby(['ResponseCode', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can apply operations to each group such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_grouped.get_group((200, '30/Apr/1998')).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.size()\n",
    "# returns a panda Series(1-d array with the groupby value(s)as index, count of each group as data values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.sum(), grouped.mean(), grouped.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can also provide useful insights of numerical columns in a DataFrame. The function describe will provide basic statistics such as count, mean, standard deviation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualize our data using PyLab (this step requires to have matplotlib installed, already installed in the virtual machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sizes of each group by ResponseCode\n",
    "grouped.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# show the percentage of each response code\n",
    "import matplotlib.pyplot as plt\n",
    "grouped.size().plot(kind='pie', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or show as bar chart\n",
    "grouped.size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the diferences between the counts are huge, use logy y-axis\n",
    "ax = grouped.size().plot(kind='bar')\n",
    "ax.set_yscale('log', nonposy='clip')# the 'noposy='clip'' is to replace the invalid log(0) with a very small positive one\n",
    "                                  # while doing the log-scale transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check how requests changes during different hour of the day on '01/May/1998'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may1_df = log_df[log_df['Date'] == '01/May/1998']\n",
    "may1_df.loc[:,('DateTime')] = pd.to_datetime(may1_df.apply(lambda row: row['Date'] + ' ' + row['Time'], axis=1))\n",
    "hour_grouped = may1_df.groupby(lambda x: may1_df['DateTime'][x].hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how many requests were made during each hour of the day on '01/May/1998' \n",
    "hour_grouped.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and how much traffic the server handled during each hour:\n",
    "hour_grouped['Size'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the changes of traffic on different hours of the day.\n",
    "ax = hour_grouped.size().plot()\n",
    "ax.set_ylabel(\"# Requests\")\n",
    "ax.set_xlabel(\"Hour of the day\")\n",
    "ax.set_title(\"# Request changes in a day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show the # requests and size of traffic in a single graph:\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "x = hour_grouped.size().index\n",
    "\n",
    "ax1.plot(x, hour_grouped.size(), 'g-')\n",
    "ax2.plot(x, hour_grouped['Size'].sum(), 'r-')\n",
    "\n",
    "ax1.set_xlabel('Hour of the day')\n",
    "ax1.set_ylabel('# Requests', color='g')\n",
    "ax2.set_ylabel('Size of traffic handled', color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it's your turn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: You don't have to submit anything for this homework.\n",
    "\n",
    "Questions:\n",
    "1. How many requests that were sent on '30/Apr/1998' and had HTTP return code 404?\n",
    "2. Generate a plot of the number of **distinct** users of the site every hour on 01/May/1998 using bar chart.\n",
    "3. Generate a line plot of the number of **distinct** users vs  size of traffic of the site every hour on 01/May/1998, is there any correlation? (hint: use \"hour_grouped['ClientID'].nunique()\" to get the number of distinct users)\n",
    "5. We wish to see if there is any correlation between client-ids and hours of the day at which they visit the website. Get 100 client ids from the dataset and generate a scatter plot that shows the hours of the day these clients sent requests. Hint: df.plot(kind='scatter', x='a', y='b'); and df['Column'].unique()\n",
    "6. The log file used in the lab was from one day of the WorldCup. Lets apply our analysis to Jul/24 and Jul/25 in the [log(wc_day91_1.log.tar.bz2)](https://ufl.instructure.com/courses/344204/files/folder/Labs/Lab%202). Repeat exercises 3 and 4 with it. How similar or different are the results? Hint: You can use UNIX command line tools from Lab 1 to first get a csv file and then load it into Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
